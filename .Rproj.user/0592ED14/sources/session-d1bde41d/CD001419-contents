
# Joseph Webb
# Honors Thesis
# March 9, 2023
#Testing change

# Load required libraries
library(tidyverse)
library(geosphere)

# Read in ForestData.csv and NEStations.csv
forest_data <- read.csv("ForestData.csv") %>% 
  select(-c(7:9)) %>% # remove empty columns
  slice(1:304) # select rows 1-304

colnames(forest_data)[5] <- "LONGITUDE"

station_data <- read.csv("NEStations.csv", header = TRUE)

# Convert latitude and longitude columns to numeric
forest_data$LATITUDE <- as.numeric(forest_data$LATITUDE)
forest_data$LONGITUDE <- as.numeric(forest_data$LONGITUDE)
station_data$LATITUDE <- as.numeric(station_data$LATITUDE)
station_data$LONGITUDE <- as.numeric(station_data$LONGITUDE)

############ FINDING CLOSEST FOREST ##################
# Define a function to find closest station
find_closest_station <- function(lat, lon, station_data) {
  distances <- geosphere::distHaversine(station_data[, c("LONGITUDE", "LATITUDE")], c(lon, lat))/1000
  closest_station_index <- which.min(distances)
  closest_station_id <- station_data$STATION_ID[closest_station_index]
  closest_station_distance <- distances[closest_station_index]
  return(list(closest_station = closest_station_id, DIST_KM = closest_station_distance))
}

# Update forest_data dataframe with closest station and distance
forest_data <- forest_data %>%
  rowwise() %>%
  mutate(CLOSE_STAT = find_closest_station(LATITUDE, LONGITUDE, station_data)$closest_station,
         DIST_KM = find_closest_station(LATITUDE, LONGITUDE, station_data)$DIST_KM)



# Round the distance measurements to one decimal place
forest_data$DIST_KM <- round(forest_data$DIST_KM, 1)

# View results
print(forest_data)
############## COMPLETE DATA SET ###############

############## Accuracy Report ###############
# Number of unique observations under CLOSE_STAT
unique_stations <- n_distinct(forest_data$CLOSE_STAT)
# Mean distance under DIST_KM
mean_distance <- mean(forest_data$DIST_KM)
# Standard deviation of distances under DIST_KM
sd_distance <- sd(forest_data$DIST_KM)
# Number of forests matched with a station farther than one standard deviation away from the mean DIST_KM
farther_than_sd <- sum(forest_data$DIST_KM > mean_distance + sd_distance | forest_data$DIST_KM < mean_distance - sd_distance)

# Create the MatchAccuracy matrix
# Number of unique observations under CLOSE_STAT
unique_stations <- n_distinct(forest_data$CLOSE_STAT)
# Mean distance under DIST_KM
mean_distance <- mean(forest_data$DIST_KM)
# Standard deviation of distances under DIST_KM
sd_distance <- sd(forest_data$DIST_KM)
# Number of forests matched with a station farther than one standard deviation away from the mean DIST_KM
farther_than_sd <- sum(forest_data$DIST_KM > mean_distance + sd_distance | forest_data$DIST_KM < mean_distance - sd_distance)

# Create the MatchAccuracy matrix
MatchAccuracy <- matrix(nrow=3, ncol=2)
colnames(MatchAccuracy) <- c("Inquiry", "Output")
MatchAccuracy[,1] <- c("Number of unique observations under CLOSE_STAT", "Mean distance under DIST_KM", "Forests matched with station farther than 1 SD from mean DIST_KM")
MatchAccuracy[,2] <- c(unique_stations, mean_distance, farther_than_sd)

library(ggplot2)

ggplot(forest_data, aes(x=DIST_KM)) + 
  geom_histogram(fill="cornflowerblue", color="black") +
  ggtitle("Distribution of Distance (km) from Forest to Closest Station") +
  xlab("Distance (km)") + ylab("Count")

################ Gathering Data From https://www.ncei.noaa.gov/data/global-summary-of-the-month/access/ ######

## Generate List of Unique Appearances in CLOSE_STAT ##
unique_stations <- unique(forest_data$CLOSE_STAT)

# Create directory if it doesn't exist
if(!dir.exists("StatData")) {
  dir.create("StatData")
}

# Set the URL for the csv files
url <- "https://www.ncei.noaa.gov/data/global-summary-of-the-month/access/"

# Loop through the station IDs and download the corresponding csv file
# for (station_id in unique_stations) {
#  file_url <- paste0(url, station_id, ".csv")
#  download.file(file_url, destfile = paste0("StatData/", station_id, ".csv"), method = "curl")
# }

### Create new matrix that combines the station data for May 2022
## find used varaibles
# Set the path to the directory containing the CSV files
path <- "StatData"

# Get a list of all the CSV files in the directory
files <- list.files(path, pattern = ".csv$")

# Initialize an empty data frame to store the frequency counts
freq_table <- data.frame(variable = character(),
                         count = numeric(),
                         stringsAsFactors = FALSE)

# Loop through the CSV files
for (file in files) {
  # Read in the data from the current file
  data <- read.csv(file.path(path, file), stringsAsFactors = FALSE)
  
  # Get the column names
  col_names <- names(data)
  
  # Remove Station, Date, Latitude, Longitude, and Name columns
  col_names <- col_names[!col_names %in% c("STATION", "DATE", "LATITUDE", "LONGITUDE", "NAME")]
  
  # Loop through the remaining columns
  for (col_name in col_names) {
    # Check if the variable is already in the frequency table
    if (col_name %in% freq_table$variable) {
      # Increment the count for the variable
      freq_table[freq_table$variable == col_name, "count"] <- freq_table[freq_table$variable == col_name, "count"] + 1
    } else {
      # Add a new row to the frequency table for the variable
      freq_table <- rbind(freq_table, data.frame(variable = col_name, count = 1, stringsAsFactors = FALSE))
    }
  }
}

freq_table <- subset(freq_table, !grepl("ATTRIBUTES$", variable))

# Print the frequency table (45 variables)
print(freq_table)

#######################  Add Station Data to Forest ##############################
# define the variables to include in the final output
vars_to_include <- c("STATION", "DATE", "ELEVATION", "DP01", "DP10", "DP1X", "DSND", "DSNW"    ,  "EMSD"      ,"EMSN"     , "EMXP"     , "PRCP",      "SNOW"      ,"CDSD"  ,    "CLDD"    ,  "DT00"  ,    "DT32"  ,   
                     "DX32"  ,    "DX70"     , "DX90"    ,  "DYFG"   ,   "DYTS"     , "EMNT"   ,  "EMXT"    ,  "HDSD"  ,   "HTDD" ,     "TAVG" ,     "TMAX"   ,   "TMIN"  ,    "EVAP",      "MNPN"   ,   "MXPN"  ,   
                     "WDMV"  ,    "AWND"   ,   "DYHF"    ,  "TSUN"    , "WDF2"  ,    "WDF5"   ,   "WSF2"  ,    "WSF5"  ,    "WDFG"  ,    "WSFG"    , "PSUN",     "WDF1"    ,  "WDFM"   ,  "WSF1"    ,  "WSFM"  )

# set the working directory to the StatData folder
setwd("C:/Users/Joe/Desktop/HonorThesis/StatData")

# create an empty data frame to store the final output
stat_data <- data.frame()

# loop through all files in the working directory
for (filename in list.files(pattern = "\\.csv$")) {
  
  # read in the csv file
  data <- read.csv(filename, header = TRUE, stringsAsFactors = FALSE)
  
  # filter the data to only include rows with DATE = "2022-05"
  data <- data[data$DATE == "2022-05", ]
  
  if (nrow(data) > 0) { # add this check
    # add columns with missing variables and set them to NA
    missing_vars <- setdiff(vars_to_include, colnames(data))
    for (var in missing_vars) {
      data[[var]] <- NA
    }
    
    # select only the variables listed in vars_to_include
    data <- data[, vars_to_include]
    
    # append the data to the stat_data data frame
    stat_data <- rbind(stat_data, data)
  }
  
}


# reset the row names of the final data frame
rownames(stat_data) <- NULL

## Retrieve missed stations
# set the working directory to the folder containing the forest_data and stat_data files
setwd("C:/Users/Joe/Desktop/HonorThesis")

# extract the list of close stations
close_stations <- forest_data$CLOSE_STAT

# find the missed stations by subtracting close_stations from the STATION variable in stat_data
missed_stations <- setdiff(stat_data$STATION, close_stations)


##################### MERGE DATA
merged_data <- merge(forest_data, stat_data, by.x = "CLOSE_STAT", by.y = "STATION", all.x = TRUE)


######################################### Export Data ###########################
setwd("C:/Users/Joe/Desktop/HonorThesis") # set working directory
write.csv(merged_data, "forest_with_statdata.csv", row.names = FALSE) # export merged_data as CSV

######################################## PCA ####################################
# Clean the data to only consider rows and columns with no NA's
# Test all columns for complete NA values
na_cols <- sapply(merged_data, function(x) all(is.na(x)))

# Get column names that satisfy the condition
na_col_names <- names(na_cols[na_cols == TRUE])

# Print column names
cat("Columns with complete NA values: ", paste(na_col_names, collapse = ", "))

## Remove Empty columns
merged_data <- merged_data[, !names(merged_data) %in% na_col_names]

################# PRECIPITATION DATA FRAME #############
prec_data <- merged_data
prec_data <- prec_data[complete.cases(prec_data), ]
prec_data <- prec_data[complete.cases(prec_data$DATE), ]

